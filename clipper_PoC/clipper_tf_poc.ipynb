{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipper Tensorflow Proof of Concept\n",
    "In this notebook, we will show you how to serve a Tensorflow model using Clipper. In the first part, we will demonstrate how to use clipper provided general Tensorflow container to launch a Tensorflow serving service. In the second part, we will guide you how to launch a serving service using your custormized container, although here we still use the same TensorFlow model, it is easy to extend to DyNet, LightGBM and other platforms.\n",
    "\n",
    "Before we start, make sure you have Clipper successfully installed in your environment. I suggest you install from source code instead of [the wheel file from PyPI](https://pypi.python.org/pypi/clipper_admin/0.2.0) since it is a little outmoded. Check out Part 0 below if you don't know how to install Clipper from source. Otherwise, skip part 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Install Clipper from Source\n",
    "```bash\n",
    "git clone https://github.com/ucbrise/clipper.git\n",
    "cd clipper/clipper_admin\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: TensorFlow Model Serving Directly\n",
    "This is the Proof of Concept of how to serve a Tensorflow Model using Clipper. Here we will use a simple Logistic Regression Model written with Tensorflow. Other tensorflow models should be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use is a image dataset contains 104 small images with 28 x 28 size. Each has a label either be 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 104 images with each size equals to 784.\n",
      "The label of the first image in the original dataset is 0.\n",
      "The label of the last image in the original dataset is 1.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def objective(y, pos_label):\n",
    "    # prediction objective\n",
    "    if y == pos_label:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def data_transformation(train_path, pos_label):\n",
    "    trainData = np.genfromtxt(train_path, delimiter=',', dtype=int)\n",
    "    records = trainData[:, 1:] \n",
    "    labels = trainData[:, :1] \n",
    "    transformedlabels = [objective(ele, pos_label) for ele in labels]\n",
    "    return (records, transformedlabels)\n",
    "\n",
    "(X_train, y_train) = data_transformation(\"train.data\", 3)\n",
    "print(\"There are %s images with each size equals to %s.\" % (len(X_train), len(X_train[0])))\n",
    "print(\"The label of the first image in the original dataset is %s.\\nThe label of the last image in the original dataset is %s.\" % (y_train[0], y_train[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-791be3ce8ece>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Cost , Accuracy\n",
      "[6659.0625, 0.63461536]\n",
      "Cost , Accuracy\n",
      "[0.0, 1.0]\n",
      "Cost , Accuracy\n",
      "[0.0, 1.0]\n",
      "Cost , Accuracy\n",
      "[0.0, 1.0]\n",
      "Cost , Accuracy\n",
      "[0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "def train_logistic_regression(X_train, y_train):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, X_train.shape[1]], name=\"pixels\")\n",
    "    y_labels = tf.placeholder(tf.int32, [None], name=\"labels\")\n",
    "    y = tf.one_hot(y_labels, depth=2)\n",
    "\n",
    "    W = tf.Variable(tf.zeros([X_train.shape[1], 2]), name=\"weights\")\n",
    "    b = tf.Variable(tf.zeros([2]), name=\"biases\")\n",
    "    y_hat = tf.matmul(x, W) + b \n",
    "\n",
    "    pred = tf.argmax(tf.nn.softmax(y_hat), 1, name=\"predict_class\")  # Softmax\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=y_hat, labels=y))\n",
    "    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hat, 1), tf.argmax(y, 1)), tf.float32))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(5000):\n",
    "        sess.run(train, feed_dict={x: X_train, y_labels: y_train})\n",
    "        if i % 1000 == 0:\n",
    "            print('Cost , Accuracy')\n",
    "            print(sess.run(\n",
    "                [loss, accuracy], feed_dict={\n",
    "                    x: X_train,\n",
    "                    y_labels: y_train\n",
    "                })) \n",
    "    return sess\n",
    "\n",
    "sess = train_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Save as Checkpoint File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "ckp_prefix = \"tf_checkpoint_file/model.ckpt\"\n",
    "save_path = saver.save(sess, ckp_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Save as SavedModel Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: frozen_graph/export_dir/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'frozen_graph/export_dir/saved_model.pb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the folder if it exists\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(\"frozen_graph/export_dir\")\n",
    "builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING])\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Clipper\n",
    "To start a clipper instance, you need to use the clipper_admin tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-04-06:16:45:00 INFO     [clipper_admin.py:1192] Stopped all Clipper cluster and all model containers\n",
      "18-04-06:16:45:00 INFO     [docker_container_manager.py:106] Starting managed Redis instance in Docker\n",
      "18-04-06:16:45:03 INFO     [clipper_admin.py:114] Clipper is running\n"
     ]
    }
   ],
   "source": [
    "from clipper_admin import ClipperConnection, DockerContainerManager\n",
    "\n",
    "clipper_conn = ClipperConnection(DockerContainerManager())\n",
    "clipper_conn.stop_all()\n",
    "# Start Clipper. Running this command for the first time will\n",
    "# download several Docker containers, so it may take some time.\n",
    "clipper_conn.start_clipper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see there are several containers started at the back.\n",
    "![pic1](./pic/clipper_start.png \"pic1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register an Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-04-06:16:47:13 INFO     [clipper_admin.py:189] Application tf-lr-app was successfully registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'tf-lr-app']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register an application called \"tf-lr-app\". This will create\n",
    "# a prediction REST endpoint at http://localhost:1337/tf-lr-app/predict\n",
    "# here 1337 is the default port inside Clipper.\n",
    "clipper_conn.register_application(name=\"tf-lr-app\",\n",
    "                                  input_type=\"integers\",\n",
    "                                  default_output=\"rabbit\",\n",
    "                                  slo_micros=100000)\n",
    "# Inspect Clipper to see the registered apps\n",
    "clipper_conn.get_all_apps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Predict Function\n",
    "Now we need to define a predict function for our logistic regression model to serve coming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the variable names are known inside your model(when defining the network) which is transparent to end users.\n",
    "# Define the predict function that returns the prediction result of each input image.\n",
    "# Note that the prediction function takes a list of input images as input and returns a list of strings.\n",
    "def predict(sess, inputs):\n",
    "    preds = sess.run('predict_class:0', feed_dict={'pixels:0': inputs})\n",
    "    return [str(p) for p in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "To deloy the TensorFlow model in Clipper, we use predefined `deploy_tensorflow_model` deployer. As you can see, there are several options below. You can deploy you predict function using checkpoint files, SavedModel format files, or a TensorFlow runtime session restored from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-04-06:16:56:52 INFO     [deployer_utils.py:49] Saving function to /tmp/clipper/tmpm1MHcG\n",
      "18-04-06:16:56:55 INFO     [deployer_utils.py:70] Warning: Anaconda environment was either not found or exporting the environment failed. Your function will still be serialized and deployed, but may fail due to missing dependencies. In this case, please re-run inside an Anaconda environment. See http://docs.clipper.ai/en/release-0.2/index.html#pure-python-functions for more information.\n",
      "18-04-06:16:57:13 INFO     [deployer_utils.py:79] Supplied local modules\n",
      "18-04-06:16:57:13 INFO     [deployer_utils.py:85] Serialized and supplied predict function\n",
      "18-04-06:16:57:13 INFO     [tensorflow.py:250] TensorFlow model copied to: tfmodel \n",
      "18-04-06:16:57:15 INFO     [clipper_admin.py:391] Building model Docker image with model data from /tmp/clipper/tmpm1MHcG\n",
      "18-04-06:16:57:20 INFO     [clipper_admin.py:395] Pushing model Docker image to tf-lr-model:1\n",
      "18-04-06:16:57:21 INFO     [docker_container_manager.py:243] Found 0 replicas for tf-lr-model:1. Adding 1\n",
      "18-04-06:16:57:21 INFO     [clipper_admin.py:569] Successfully registered model tf-lr-model:1\n",
      "18-04-06:16:57:21 INFO     [clipper_admin.py:487] Done deploying model tf-lr-model:1.\n",
      "18-04-06:16:57:22 INFO     [clipper_admin.py:232] Model tf-lr-model is now linked to application tf-lr-app\n"
     ]
    }
   ],
   "source": [
    "app_name = \"tf-lr-app\"\n",
    "model_name = \"tf-lr-model\"\n",
    "from clipper_admin.deployers.tensorflow import deploy_tensorflow_model\n",
    "# option 1: deloy predict function using SavedModel format files\n",
    "deploy_tensorflow_model(clipper_conn,\n",
    "                        model_name,\n",
    "                        version=1,\n",
    "                        input_type=\"integers\",\n",
    "                        func=predict,\n",
    "                        tf_sess_or_saved_model_path=\"frozen_graph/export_dir\")\n",
    "# option 2: deploy predic function using checkpoint files\n",
    "# Note in this case tf_sess_or_saved_model_path should be the prefix folder name for the ckp files: tf_checkpoint_file instead of tf_checkpoint_file/model.ckpt\n",
    "'''\n",
    "deploy_tensorflow_model(clipper_conn,\n",
    "                        model_name,\n",
    "                        version=2,\n",
    "                        input_type=\"integers\",\n",
    "                        func=predict,\n",
    "                        tf_sess_or_saved_model_path=\"tf_checkpoint_file\")\n",
    "'''\n",
    "# option 3: restore sess from ckp file\n",
    "# TODO\n",
    "# option 4: restore sess from SavedModel\n",
    "# TODO\n",
    "\n",
    "# link the app with your deployed model\n",
    "clipper_conn.link_model_to_app(app_name, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you can see, Clipper started a new container named `tf-lr-model_1-37073` at the back.\n",
    "![pic2](./pic/after_deploy.png \"pic2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Predictions\n",
    "For simplicity, we'll generate a random image to request the prediction. It should work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:1337\n",
      "{u'default': False, u'output': 1, u'query_id': 0}\n",
      "{u'default': False, u'output': 1, u'query_id': 1}\n",
      "{u'default': False, u'output': 1, u'query_id': 2}\n",
      "{u'default': False, u'output': 1, u'query_id': 3}\n",
      "{u'default': False, u'output': 1, u'query_id': 4}\n",
      "{u'default': False, u'output': 1, u'query_id': 5}\n",
      "{u'default': False, u'output': 1, u'query_id': 6}\n",
      "{u'default': False, u'output': 1, u'query_id': 7}\n",
      "{u'default': False, u'output': 1, u'query_id': 8}\n",
      "{u'default': False, u'output': 1, u'query_id': 9}\n",
      "{u'default': False, u'output': 1, u'query_id': 10}\n",
      "{u'default': False, u'output': 1, u'query_id': 11}\n",
      "{u'default': False, u'output': 1, u'query_id': 12}\n",
      "{u'default': False, u'output': 1, u'query_id': 13}\n",
      "{u'default': False, u'output': 1, u'query_id': 14}\n",
      "{u'default': False, u'output': 1, u'query_id': 15}\n",
      "{u'default': False, u'output': 1, u'query_id': 16}\n",
      "{u'default': False, u'output': 1, u'query_id': 17}\n",
      "{u'default': False, u'output': 1, u'query_id': 18}\n",
      "{u'default': False, u'output': 1, u'query_id': 19}\n",
      "{u'default': False, u'output': 1, u'query_id': 20}\n",
      "{u'default': False, u'output': 1, u'query_id': 21}\n",
      "{u'default': False, u'output': 1, u'query_id': 22}\n",
      "{u'default': False, u'output': 1, u'query_id': 23}\n",
      "{u'default': False, u'output': 1, u'query_id': 24}\n"
     ]
    }
   ],
   "source": [
    "headers = {'Content-type': 'application/json'}\n",
    "\n",
    "def get_test_point():\n",
    "    return [np.random.randint(255) for _ in range(784)]\n",
    "\n",
    "def test_model(clipper_conn, app, version):\n",
    "    time.sleep(25)\n",
    "    num_preds = 25\n",
    "    num_defaults = 0 \n",
    "    addr = clipper_conn.get_query_addr()\n",
    "    print(addr)\n",
    "    for i in range(num_preds):\n",
    "        response = requests.post(\n",
    "            \"http://%s/%s/predict\" % (addr, app),\n",
    "            headers=headers,\n",
    "            data=json.dumps({\n",
    "                'input': get_test_point()\n",
    "            })) \n",
    "        result = response.json()\n",
    "        print(result)\n",
    "        if response.status_code == requests.codes.ok and result[\"default\"]:\n",
    "            num_defaults += 1\n",
    "        elif response.status_code != requests.codes.ok:\n",
    "            print(result)\n",
    "            raise BenchmarkException(response.text)\n",
    "\n",
    "    if num_defaults > 0:\n",
    "        print(\"Error: %d/%d predictions were default\" % (num_defaults,\n",
    "                                                         num_preds))\n",
    "\n",
    "# You need to reconnect to the clipper instance in another process\n",
    "# clipper_conn.connect()\n",
    "test_model(clipper_conn, \"tf-lr-app\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Customized TensorFlow Model Serving\n",
    "In this section, we will guide you how to launch a serving service with your own docker images. For example, you want to serve a DyNet model. But for consistency, we'll still use above TensorFlow's logistic regression example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Service\n",
    "Instead of just writing a predict function, now you need to write a prediction service. Clipper uses rpc for communication between Clipper and your predict container. Clipper's python [rpc implementation](https://github.com/ucbrise/clipper/blob/develop/containers/python/rpc.py) has very simple interface, let's check out the example below. We'll use checkpoint files as input to start a predict service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rpc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3647d35a92e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTFLRContainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelContainerBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rpc' is not defined"
     ]
    }
   ],
   "source": [
    "# tf_lr_container.py\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import rpc # to use this, you can use the base image(https://hub.docker.com/r/clipper/py-rpc/) to build your container \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class TFLRContainer(rpc.ModelContainerBase):\n",
    "    def __init__(self, path):\n",
    "        self.sess = tf.Session('', tf.Graph())\n",
    "        with self.sess.graph.as_default():\n",
    "            saver = tf.train.import_meta_graph(path + '.meta')\n",
    "\n",
    "    def predict_ints(self, inputs):\n",
    "        preds = self.sess.run('predict_class:0', feed_dict = {'pixels:0': inputs})\n",
    "        return [str(pred) for pred in preds]\n",
    "\n",
    "\n",
    "def service():\n",
    "    print('Starting TensorFlow LR container')\n",
    "    model_name = os.environ[\"CLIPPER_MODEL_NAME\"] # dynamic pass to the container\n",
    "    model_version = os.environ[\"CLIPPER_MODEL_VERSION\"] # dynamic pass to the container\n",
    "    ip = \"127.0.0.1\"\n",
    "    port = 7000\n",
    "\n",
    "    input_type = \"ints\"\n",
    "    model_dir_path = os.environ[\"CLIPPER_MODEL_PATH\"] # \"/model\" by default \n",
    "    model_files = os.listdir(model_dir_path)\n",
    "    assert len(model_files) >= 2\n",
    "    fname = os.path.splitext(model_files[0])[0]\n",
    "    full_fname = os.path.join(model_dir_path, fname)\n",
    "    print(full_fname)\n",
    "    model = TFLRContainer(full_fname)\n",
    "\n",
    "    # start rpc service using TFLRContainer\n",
    "    rpc_service = rpc.RPCService()\n",
    "    rpc_service.start(model, ip, port, model_name, model_version, input_type)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entry Point\n",
    "Also, let's create a entry point for our container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```code\n",
    "# tf_lr_container_entry.sh\n",
    "#!/usr/bin/env sh\n",
    "\n",
    "IMPORT_ERROR_RETURN_CODE=3\n",
    "\n",
    "echo \"Attempting to run TensorFlow container without installing any dependencies\"\n",
    "echo \"Contents of /model\"\n",
    "ls /model/\n",
    "\n",
    "/bin/bash -c \"exec python /container/tf_lr_container.py\"\n",
    "if [ $? -eq $IMPORT_ERROR_RETURN_CODE ]; then\n",
    "  echo \"Running TensorFlow container without installing dependencies fails\"\n",
    "  echo \"Will install dependencies and try again\"\n",
    "  conda install -y --file /model/conda_dependencies.txt\n",
    "  pip install -r /model/pip_dependencies.txt\n",
    "  /bin/bash -c \"exec python /container/tf_lr_container.py\"\n",
    "fi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DockerFile\n",
    "Now we can write our dockerfile finally :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```DockerFile\n",
    "# TensorFlowLRDockerfile\n",
    "FROM clipper/py-rpc:09dfc97\n",
    "\n",
    "COPY python_container_conda_deps.txt /lib/\n",
    "\n",
    "RUN conda config --set ssl_verify no \\\n",
    "  && conda install -c anaconda cloudpickle=0.5.2 \\\n",
    "  && conda install -y --file /lib/python_container_conda_deps.txt \\\n",
    "  && conda install tensorflow\n",
    "\n",
    "COPY tf_lr_container.py tf_lr_container_entry.sh /container/\n",
    "\n",
    "CMD [\"/container/tf_lr_container_entry.sh\"]\n",
    "\n",
    "# vim: set filetype=dockerfile:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-04-06:11:49:27 ERROR    [execution.py:622] File `u'docker.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "# Now you can build the container and push it to your docker registry\n",
    "%run docker build xunzhang/tf_lr_container:latest -f TensorFlowLRDockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "We will use `clipper_conn.build_and_deploy_model` to deploy the service. The Docker container will load and reconstruct the model from the serialized model checkpoint when the container is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=3,\n",
    "    input_type=\"ints\",\n",
    "    model_data_path=os.path.abspath(\"tf_checkpoint_file\"),\n",
    "    base_image=\"xunzhang/tf_lr_container:latest\",\n",
    "    num_replicas=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
